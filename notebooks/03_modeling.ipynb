{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Food Adverse Events - Anomaly Detection Modeling\n",
    "\n",
    "This notebook implements and compares multiple anomaly detection approaches for identifying potential foodborne illness outbreaks:\n",
    "\n",
    "1. **Statistical Methods**: Z-score, Moving Average with thresholds\n",
    "2. **Machine Learning**: Isolation Forest\n",
    "3. **Time Series Forecasting**: ARIMA, Prophet with residual analysis\n",
    "\n",
    "**Goal**: Identify unusual spikes in adverse reactions that may indicate outbreak events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and ML libraries\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load time series data\n",
    "ts_daily_train = pd.read_parquet('../data/processed/timeseries_daily_train.parquet')\n",
    "ts_daily_test = pd.read_parquet('../data/processed/timeseries_daily_test.parquet')\n",
    "ts_weekly_train = pd.read_parquet('../data/processed/timeseries_weekly_train.parquet')\n",
    "ts_weekly_test = pd.read_parquet('../data/processed/timeseries_weekly_test.parquet')\n",
    "\n",
    "# Load list of reactions\n",
    "import json\n",
    "with open('../data/processed/top_reactions.json', 'r') as f:\n",
    "    top_reactions = json.load(f)\n",
    "\n",
    "print(f\"Loaded time series data:\")\n",
    "print(f\"  Daily train: {ts_daily_train.shape}\")\n",
    "print(f\"  Daily test: {ts_daily_test.shape}\")\n",
    "print(f\"  Weekly train: {ts_weekly_train.shape}\")\n",
    "print(f\"  Weekly test: {ts_weekly_test.shape}\")\n",
    "print(f\"\\nTracking {len(top_reactions)} reactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline: Statistical Anomaly Detection\n",
    "\n",
    "### 2.1 Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_zscore(series, threshold=3, window=7):\n",
    "    \"\"\"\n",
    "    Detect anomalies using rolling Z-score\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        threshold: Number of standard deviations for anomaly\n",
    "        window: Rolling window size\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with anomaly flags and scores\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=series.index)\n",
    "    result['value'] = series.values\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    result['rolling_mean'] = series.rolling(window=window, min_periods=1).mean()\n",
    "    result['rolling_std'] = series.rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # Calculate Z-score\n",
    "    result['zscore'] = (result['value'] - result['rolling_mean']) / (result['rolling_std'] + 1e-10)\n",
    "    \n",
    "    # Flag anomalies\n",
    "    result['is_anomaly'] = (np.abs(result['zscore']) > threshold)\n",
    "    result['anomaly_score'] = np.abs(result['zscore'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test on first reaction\n",
    "reaction = top_reactions[0]\n",
    "series = ts_weekly_train[reaction]\n",
    "\n",
    "anomalies_zscore = detect_anomalies_zscore(series, threshold=3, window=4)\n",
    "print(f\"Z-score anomaly detection for '{reaction}':\")\n",
    "print(f\"  Total data points: {len(anomalies_zscore)}\")\n",
    "print(f\"  Anomalies detected: {anomalies_zscore['is_anomaly'].sum()}\")\n",
    "print(f\"  Anomaly rate: {anomalies_zscore['is_anomaly'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Z-score anomalies\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Time series with anomalies\n",
    "axes[0].plot(anomalies_zscore.index, anomalies_zscore['value'], \n",
    "            label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(anomalies_zscore.index, anomalies_zscore['rolling_mean'], \n",
    "            label='Rolling Mean', linewidth=2, color='orange', linestyle='--')\n",
    "\n",
    "# Highlight anomalies\n",
    "anomaly_points = anomalies_zscore[anomalies_zscore['is_anomaly']]\n",
    "axes[0].scatter(anomaly_points.index, anomaly_points['value'], \n",
    "               color='red', s=100, label='Anomalies', zorder=5)\n",
    "\n",
    "axes[0].set_title(f\"Z-Score Anomaly Detection: {reaction.title()}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Z-scores\n",
    "axes[1].plot(anomalies_zscore.index, anomalies_zscore['zscore'], \n",
    "            linewidth=2, color='purple')\n",
    "axes[1].axhline(y=3, color='red', linestyle='--', label='±3σ threshold')\n",
    "axes[1].axhline(y=-3, color='red', linestyle='--')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].fill_between(anomalies_zscore.index, -3, 3, alpha=0.1, color='green')\n",
    "axes[1].set_title('Z-Scores Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Z-Score')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Moving Average with Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_moving_avg(series, window=7, threshold_multiplier=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using moving average with threshold\n",
    "    \n",
    "    Args:\n",
    "        series: Time series data\n",
    "        window: Rolling window size\n",
    "        threshold_multiplier: Multiplier for std to set threshold\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with anomaly flags\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=series.index)\n",
    "    result['value'] = series.values\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    result['rolling_mean'] = series.rolling(window=window, min_periods=1).mean()\n",
    "    result['rolling_std'] = series.rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "    # Calculate upper threshold\n",
    "    result['upper_threshold'] = result['rolling_mean'] + (threshold_multiplier * result['rolling_std'])\n",
    "    result['lower_threshold'] = result['rolling_mean'] - (threshold_multiplier * result['rolling_std'])\n",
    "    \n",
    "    # Flag anomalies (values above upper threshold)\n",
    "    result['is_anomaly'] = (result['value'] > result['upper_threshold']) | \\\n",
    "                          (result['value'] < result['lower_threshold'])\n",
    "    \n",
    "    # Anomaly score (distance from mean in terms of std)\n",
    "    result['anomaly_score'] = np.abs(result['value'] - result['rolling_mean']) / (result['rolling_std'] + 1e-10)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply to same reaction\n",
    "anomalies_ma = detect_anomalies_moving_avg(series, window=4, threshold_multiplier=2.5)\n",
    "print(f\"Moving Average anomaly detection for '{reaction}':\")\n",
    "print(f\"  Total data points: {len(anomalies_ma)}\")\n",
    "print(f\"  Anomalies detected: {anomalies_ma['is_anomaly'].sum()}\")\n",
    "print(f\"  Anomaly rate: {anomalies_ma['is_anomaly'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize moving average anomalies\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(anomalies_ma.index, anomalies_ma['value'], \n",
    "        label='Actual', linewidth=2, alpha=0.7)\n",
    "plt.plot(anomalies_ma.index, anomalies_ma['rolling_mean'], \n",
    "        label='Rolling Mean', linewidth=2, color='orange', linestyle='--')\n",
    "plt.fill_between(anomalies_ma.index, \n",
    "                anomalies_ma['lower_threshold'], \n",
    "                anomalies_ma['upper_threshold'],\n",
    "                alpha=0.2, color='green', label='Normal Range')\n",
    "\n",
    "# Highlight anomalies\n",
    "anomaly_points = anomalies_ma[anomalies_ma['is_anomaly']]\n",
    "plt.scatter(anomaly_points.index, anomaly_points['value'], \n",
    "           color='red', s=100, label='Anomalies', zorder=5)\n",
    "\n",
    "plt.title(f\"Moving Average Anomaly Detection: {reaction.title()}\", \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning: Isolation Forest\n",
    "\n",
    "Isolation Forest is effective for multivariate anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_isolation_forest(train_data, test_data, contamination=0.05, n_features=5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Isolation Forest\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training time series (DataFrame with multiple reactions)\n",
    "        test_data: Test time series\n",
    "        contamination: Expected proportion of anomalies\n",
    "        n_features: Number of top reactions to use as features\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with anomaly predictions and scores\n",
    "    \"\"\"\n",
    "    # Select top N reactions as features\n",
    "    features = train_data.columns[:n_features]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_data[features].values\n",
    "    X_test = test_data[features].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Isolation Forest\n",
    "    model = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    model.fit(X_train_scaled)\n",
    "    \n",
    "    # Predict on test data\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    anomaly_scores = -model.score_samples(X_test_scaled)  # Negative for intuitive interpretation\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result = pd.DataFrame(index=test_data.index)\n",
    "    result['is_anomaly'] = (predictions == -1)  # -1 indicates anomaly\n",
    "    result['anomaly_score'] = anomaly_scores\n",
    "    \n",
    "    # Add original values for reference\n",
    "    for feature in features:\n",
    "        result[feature] = test_data[feature].values\n",
    "    \n",
    "    return result, model, scaler, features\n",
    "\n",
    "# Apply Isolation Forest\n",
    "print(\"Training Isolation Forest...\")\n",
    "if_results, if_model, if_scaler, if_features = detect_anomalies_isolation_forest(\n",
    "    ts_weekly_train, ts_weekly_test, contamination=0.05, n_features=5\n",
    ")\n",
    "\n",
    "print(f\"\\nIsolation Forest Results:\")\n",
    "print(f\"  Features used: {list(if_features)}\")\n",
    "print(f\"  Test data points: {len(if_results)}\")\n",
    "print(f\"  Anomalies detected: {if_results['is_anomaly'].sum()}\")\n",
    "print(f\"  Anomaly rate: {if_results['is_anomaly'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Isolation Forest results\n",
    "fig, axes = plt.subplots(len(if_features), 1, figsize=(15, 4*len(if_features)))\n",
    "\n",
    "for idx, feature in enumerate(if_features):\n",
    "    # Plot time series\n",
    "    axes[idx].plot(if_results.index, if_results[feature], \n",
    "                  label='Actual', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Highlight detected anomalies\n",
    "    anomaly_points = if_results[if_results['is_anomaly']]\n",
    "    axes[idx].scatter(anomaly_points.index, anomaly_points[feature], \n",
    "                     color='red', s=100, label='Anomalies (IF)', zorder=5)\n",
    "    \n",
    "    axes[idx].set_title(f\"Isolation Forest Detection: {feature.title()}\", \n",
    "                       fontweight='bold')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly scores\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(if_results.index, if_results['anomaly_score'], \n",
    "        linewidth=2, color='purple')\n",
    "plt.scatter(anomaly_points.index, \n",
    "           if_results.loc[anomaly_points.index, 'anomaly_score'],\n",
    "           color='red', s=100, label='Detected Anomalies', zorder=5)\n",
    "plt.title('Isolation Forest - Anomaly Scores Over Time', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.xlabel('Date')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Forecasting: ARIMA\n",
    "\n",
    "Use ARIMA to forecast expected values and detect anomalies based on residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_arima(train_series, test_series, order=(1,1,1), threshold=2.5):\n",
    "    \"\"\"\n",
    "    Detect anomalies using ARIMA residual analysis\n",
    "    \n",
    "    Args:\n",
    "        train_series: Training time series\n",
    "        test_series: Test time series\n",
    "        order: ARIMA order (p, d, q)\n",
    "        threshold: Threshold for residual anomaly detection\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions, residuals, and anomalies\n",
    "    \"\"\"\n",
    "    # Fit ARIMA model\n",
    "    print(f\"Fitting ARIMA{order}...\")\n",
    "    model = ARIMA(train_series, order=order)\n",
    "    fitted_model = model.fit()\n",
    "    \n",
    "    # Forecast on test data\n",
    "    forecast_result = fitted_model.forecast(steps=len(test_series))\n",
    "    predictions = pd.Series(forecast_result, index=test_series.index)\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = test_series - predictions\n",
    "    \n",
    "    # Calculate threshold based on training residuals\n",
    "    train_residuals = fitted_model.resid\n",
    "    residual_mean = train_residuals.mean()\n",
    "    residual_std = train_residuals.std()\n",
    "    \n",
    "    # Create result dataframe\n",
    "    result = pd.DataFrame(index=test_series.index)\n",
    "    result['actual'] = test_series.values\n",
    "    result['predicted'] = predictions.values\n",
    "    result['residual'] = residuals.values\n",
    "    result['residual_zscore'] = (residuals - residual_mean) / (residual_std + 1e-10)\n",
    "    result['is_anomaly'] = (np.abs(result['residual_zscore']) > threshold)\n",
    "    result['anomaly_score'] = np.abs(result['residual_zscore'])\n",
    "    \n",
    "    print(f\"Model AIC: {fitted_model.aic:.2f}\")\n",
    "    print(f\"Model BIC: {fitted_model.bic:.2f}\")\n",
    "    \n",
    "    return result, fitted_model\n",
    "\n",
    "# Apply ARIMA to first reaction\n",
    "reaction = top_reactions[0]\n",
    "train_series = ts_weekly_train[reaction]\n",
    "test_series = ts_weekly_test[reaction]\n",
    "\n",
    "print(f\"\\nARIMA anomaly detection for '{reaction}':\")\n",
    "arima_results, arima_model = detect_anomalies_arima(\n",
    "    train_series, test_series, order=(2,1,2), threshold=2.5\n",
    ")\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Test data points: {len(arima_results)}\")\n",
    "print(f\"  Anomalies detected: {arima_results['is_anomaly'].sum()}\")\n",
    "print(f\"  Anomaly rate: {arima_results['is_anomaly'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ARIMA results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Actual vs Predicted\n",
    "axes[0].plot(arima_results.index, arima_results['actual'], \n",
    "            label='Actual', linewidth=2, alpha=0.7)\n",
    "axes[0].plot(arima_results.index, arima_results['predicted'], \n",
    "            label='ARIMA Forecast', linewidth=2, color='orange', linestyle='--')\n",
    "\n",
    "# Highlight anomalies\n",
    "anomaly_points = arima_results[arima_results['is_anomaly']]\n",
    "axes[0].scatter(anomaly_points.index, anomaly_points['actual'], \n",
    "               color='red', s=100, label='Anomalies', zorder=5)\n",
    "\n",
    "axes[0].set_title(f\"ARIMA Anomaly Detection: {reaction.title()}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "axes[1].plot(arima_results.index, arima_results['residual'], \n",
    "            linewidth=2, color='purple', label='Residuals')\n",
    "axes[1].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[1].scatter(anomaly_points.index, \n",
    "               arima_results.loc[anomaly_points.index, 'residual'],\n",
    "               color='red', s=100, label='Anomalies', zorder=5)\n",
    "axes[1].set_title('ARIMA Residuals', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply All Methods to Multiple Reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare methods on top 3 reactions\n",
    "reactions_to_compare = top_reactions[:3]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for reaction in reactions_to_compare:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {reaction.title()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    series_train = ts_weekly_train[reaction]\n",
    "    series_test = ts_weekly_test[reaction]\n",
    "    \n",
    "    # Z-score\n",
    "    test_full = pd.concat([series_train, series_test])\n",
    "    zscore_full = detect_anomalies_zscore(test_full, threshold=3, window=4)\n",
    "    zscore_test = zscore_full.loc[series_test.index]\n",
    "    \n",
    "    # Moving Average\n",
    "    ma_full = detect_anomalies_moving_avg(test_full, window=4, threshold_multiplier=2.5)\n",
    "    ma_test = ma_full.loc[series_test.index]\n",
    "    \n",
    "    # ARIMA\n",
    "    try:\n",
    "        arima_test, _ = detect_anomalies_arima(series_train, series_test, order=(2,1,2), threshold=2.5)\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA failed: {e}\")\n",
    "        arima_test = None\n",
    "    \n",
    "    # Store results\n",
    "    comparison_results[reaction] = {\n",
    "        'zscore': zscore_test['is_anomaly'].sum(),\n",
    "        'moving_avg': ma_test['is_anomaly'].sum(),\n",
    "        'arima': arima_test['is_anomaly'].sum() if arima_test is not None else None\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nAnomaly counts:\")\n",
    "    print(f\"  Z-Score: {comparison_results[reaction]['zscore']}\")\n",
    "    print(f\"  Moving Avg: {comparison_results[reaction]['moving_avg']}\")\n",
    "    if comparison_results[reaction]['arima'] is not None:\n",
    "        print(f\"  ARIMA: {comparison_results[reaction]['arima']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "comparison_df = pd.DataFrame(comparison_results).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "comparison_df.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Anomaly Detection Methods Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Reaction Type')\n",
    "ax.set_ylabel('Number of Anomalies Detected')\n",
    "ax.legend(title='Method', labels=['Z-Score', 'Moving Average', 'ARIMA'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ensemble Approach\n",
    "\n",
    "Combine multiple methods for more robust detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_anomaly_detection(reaction, train_data, test_data, min_votes=2):\n",
    "    \"\"\"\n",
    "    Ensemble anomaly detection using multiple methods\n",
    "    \n",
    "    Args:\n",
    "        reaction: Reaction name\n",
    "        train_data: Training dataframe\n",
    "        test_data: Test dataframe\n",
    "        min_votes: Minimum number of methods that must agree\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ensemble results\n",
    "    \"\"\"\n",
    "    series_train = train_data[reaction]\n",
    "    series_test = test_data[reaction]\n",
    "    \n",
    "    # Combine train and test for rolling methods\n",
    "    series_full = pd.concat([series_train, series_test])\n",
    "    \n",
    "    # Method 1: Z-score\n",
    "    zscore_full = detect_anomalies_zscore(series_full, threshold=3, window=4)\n",
    "    zscore_test = zscore_full.loc[series_test.index]\n",
    "    \n",
    "    # Method 2: Moving Average\n",
    "    ma_full = detect_anomalies_moving_avg(series_full, window=4, threshold_multiplier=2.5)\n",
    "    ma_test = ma_full.loc[series_test.index]\n",
    "    \n",
    "    # Method 3: ARIMA\n",
    "    try:\n",
    "        arima_test, _ = detect_anomalies_arima(series_train, series_test, order=(2,1,2), threshold=2.5)\n",
    "        has_arima = True\n",
    "    except:\n",
    "        has_arima = False\n",
    "    \n",
    "    # Create ensemble result\n",
    "    result = pd.DataFrame(index=series_test.index)\n",
    "    result['actual'] = series_test.values\n",
    "    result['zscore_anomaly'] = zscore_test['is_anomaly'].values\n",
    "    result['ma_anomaly'] = ma_test['is_anomaly'].values\n",
    "    \n",
    "    if has_arima:\n",
    "        result['arima_anomaly'] = arima_test['is_anomaly'].values\n",
    "        result['vote_count'] = result[['zscore_anomaly', 'ma_anomaly', 'arima_anomaly']].sum(axis=1)\n",
    "    else:\n",
    "        result['vote_count'] = result[['zscore_anomaly', 'ma_anomaly']].sum(axis=1)\n",
    "    \n",
    "    result['ensemble_anomaly'] = (result['vote_count'] >= min_votes)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply ensemble to first reaction\n",
    "reaction = top_reactions[0]\n",
    "ensemble_result = ensemble_anomaly_detection(\n",
    "    reaction, ts_weekly_train, ts_weekly_test, min_votes=2\n",
    ")\n",
    "\n",
    "print(f\"Ensemble Anomaly Detection for '{reaction}':\")\n",
    "print(f\"  Test points: {len(ensemble_result)}\")\n",
    "print(f\"  Ensemble anomalies: {ensemble_result['ensemble_anomaly'].sum()}\")\n",
    "print(f\"\\nVote distribution:\")\n",
    "print(ensemble_result['vote_count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Time series with ensemble anomalies\n",
    "axes[0].plot(ensemble_result.index, ensemble_result['actual'], \n",
    "            label='Actual', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Highlight different vote levels with different colors\n",
    "for votes in range(1, ensemble_result['vote_count'].max() + 1):\n",
    "    vote_points = ensemble_result[ensemble_result['vote_count'] == votes]\n",
    "    if len(vote_points) > 0:\n",
    "        colors = ['yellow', 'orange', 'red']\n",
    "        axes[0].scatter(vote_points.index, vote_points['actual'], \n",
    "                       color=colors[min(votes-1, 2)], s=100, \n",
    "                       label=f'{votes} votes', zorder=5, alpha=0.7)\n",
    "\n",
    "axes[0].set_title(f\"Ensemble Anomaly Detection: {reaction.title()}\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Vote counts over time\n",
    "axes[1].bar(ensemble_result.index, ensemble_result['vote_count'], \n",
    "           width=5, color='steelblue', alpha=0.7)\n",
    "axes[1].axhline(y=2, color='red', linestyle='--', label='Ensemble Threshold (2 votes)')\n",
    "axes[1].set_title('Method Agreement Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Methods Detecting Anomaly')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('../results/models', exist_ok=True)\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "\n",
    "# Save Isolation Forest model\n",
    "with open('../results/models/isolation_forest.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': if_model,\n",
    "        'scaler': if_scaler,\n",
    "        'features': if_features\n",
    "    }, f)\n",
    "print(\"Saved: Isolation Forest model\")\n",
    "\n",
    "# Save ensemble results for top reactions\n",
    "ensemble_all = {}\n",
    "for reaction in top_reactions[:5]:\n",
    "    try:\n",
    "        ensemble_all[reaction] = ensemble_anomaly_detection(\n",
    "            reaction, ts_weekly_train, ts_weekly_test, min_votes=2\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {reaction}: {e}\")\n",
    "\n",
    "# Save to parquet\n",
    "for reaction, result in ensemble_all.items():\n",
    "    safe_name = reaction.replace(' ', '_').replace('/', '_')\n",
    "    result.to_parquet(f'../results/ensemble_{safe_name}.parquet')\n",
    "print(f\"\\nSaved ensemble results for {len(ensemble_all)} reactions\")\n",
    "\n",
    "print(\"\\n✓ All results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODELING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. METHODS IMPLEMENTED:\")\n",
    "print(\"   ✓ Z-Score anomaly detection\")\n",
    "print(\"   ✓ Moving Average with thresholds\")\n",
    "print(\"   ✓ Isolation Forest (multivariate)\")\n",
    "print(\"   ✓ ARIMA with residual analysis\")\n",
    "print(\"   ✓ Ensemble approach (voting)\")\n",
    "\n",
    "print(\"\\n2. KEY FINDINGS:\")\n",
    "print(\"   - Different methods detect different types of anomalies\")\n",
    "print(\"   - Statistical methods good for univariate spikes\")\n",
    "print(\"   - Isolation Forest captures multivariate patterns\")\n",
    "print(\"   - ARIMA effective when strong temporal patterns exist\")\n",
    "print(\"   - Ensemble reduces false positives\")\n",
    "\n",
    "print(\"\\n3. NEXT STEPS:\")\n",
    "print(\"   1. Validate against known FDA recalls/outbreaks\")\n",
    "print(\"   2. Calculate precision, recall, F1-score if ground truth available\")\n",
    "print(\"   3. Tune hyperparameters for each method\")\n",
    "print(\"   4. Implement real-time detection pipeline\")\n",
    "print(\"   5. Create interactive dashboard for visualization\")\n",
    "print(\"   6. Add product-specific anomaly detection\")\n",
    "print(\"   7. Integrate geographic information if available\")\n",
    "\n",
    "print(\"\\n4. DEPLOYMENT CONSIDERATIONS:\")\n",
    "print(\"   - Set up automated data refresh pipeline\")\n",
    "print(\"   - Implement alerting system for high-confidence anomalies\")\n",
    "print(\"   - Create API for real-time queries\")\n",
    "print(\"   - Build web dashboard for stakeholders\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
