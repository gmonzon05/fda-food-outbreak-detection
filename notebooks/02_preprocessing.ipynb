{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDA Food Adverse Events - Data Preprocessing\n",
    "\n",
    "This notebook handles data cleaning, transformation, and feature engineering to prepare the dataset for anomaly detection.\n",
    "\n",
    "**Goals**:\n",
    "1. Clean and standardize the data\n",
    "2. Handle missing values appropriately\n",
    "3. Create time series aggregations by reaction type\n",
    "4. Engineer features for anomaly detection\n",
    "5. Prepare train/test splits for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_loader import FDADataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the full dataset (or a large sample) for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = '../data/raw/food-event-0001-of-0001.json'\n",
    "loader = FDADataLoader(data_path)\n",
    "\n",
    "# Adjust max_records based on your memory constraints\n",
    "# Use None to load all data\n",
    "print(\"Loading data...\")\n",
    "df = loader.load_to_dataframe(max_records=100000)  # Adjust as needed\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate report numbers\n",
    "print(f\"Records before deduplication: {len(df):,}\")\n",
    "df = df.drop_duplicates(subset=['report_number'], keep='first')\n",
    "print(f\"Records after deduplication: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean date column\n",
    "df['date_started'] = pd.to_datetime(df['date_started'], errors='coerce')\n",
    "\n",
    "# Remove records with missing dates (critical for time series)\n",
    "print(f\"Records before removing null dates: {len(df):,}\")\n",
    "df = df.dropna(subset=['date_started'])\n",
    "print(f\"Records after removing null dates: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier dates (future dates or very old dates)\n",
    "current_date = pd.Timestamp.now()\n",
    "min_date = pd.Timestamp('2000-01-01')  # Reasonable cutoff\n",
    "\n",
    "print(f\"Date range before filtering: {df['date_started'].min()} to {df['date_started'].max()}\")\n",
    "df = df[(df['date_started'] >= min_date) & (df['date_started'] <= current_date)]\n",
    "print(f\"Date range after filtering: {df['date_started'].min()} to {df['date_started'].max()}\")\n",
    "print(f\"Records after date filtering: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean age data\n",
    "# Remove unrealistic ages\n",
    "df.loc[(df['consumer_age'] < 0) | (df['consumer_age'] > 120), 'consumer_age'] = np.nan\n",
    "\n",
    "print(f\"Age range after cleaning: {df['consumer_age'].min():.1f} to {df['consumer_age'].max():.1f}\")\n",
    "print(f\"Records with valid age: {df['consumer_age'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize reactions (ensure all are lists)\n",
    "def standardize_reactions(reactions):\n",
    "    if pd.isna(reactions):\n",
    "        return []\n",
    "    if isinstance(reactions, list):\n",
    "        return [r.strip().lower() for r in reactions if r]\n",
    "    if isinstance(reactions, str):\n",
    "        return [reactions.strip().lower()]\n",
    "    return []\n",
    "\n",
    "df['reactions_clean'] = df['reactions'].apply(standardize_reactions)\n",
    "\n",
    "# Remove records with no reactions\n",
    "print(f\"Records before removing empty reactions: {len(df):,}\")\n",
    "df = df[df['reactions_clean'].apply(len) > 0]\n",
    "print(f\"Records after removing empty reactions: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "df['year'] = df['date_started'].dt.year\n",
    "df['month'] = df['date_started'].dt.month\n",
    "df['quarter'] = df['date_started'].dt.quarter\n",
    "df['day_of_week'] = df['date_started'].dt.dayofweek\n",
    "df['day_of_year'] = df['date_started'].dt.dayofyear\n",
    "df['week_of_year'] = df['date_started'].dt.isocalendar().week\n",
    "\n",
    "# Create date-only column for grouping\n",
    "df['date'] = df['date_started'].dt.date\n",
    "\n",
    "print(\"Temporal features created:\")\n",
    "print(df[['date_started', 'year', 'month', 'quarter', 'day_of_week', 'week_of_year']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reaction count feature\n",
    "df['num_reactions'] = df['reactions_clean'].apply(len)\n",
    "\n",
    "# Create outcome severity flags\n",
    "def extract_outcome_severity(outcomes):\n",
    "    if pd.isna(outcomes):\n",
    "        return {'serious': False, 'hospitalization': False, 'death': False}\n",
    "    \n",
    "    if isinstance(outcomes, str):\n",
    "        outcomes = [outcomes]\n",
    "    \n",
    "    outcomes_lower = [o.lower() if isinstance(o, str) else '' for o in outcomes]\n",
    "    \n",
    "    return {\n",
    "        'serious': any('serious' in o for o in outcomes_lower),\n",
    "        'hospitalization': any('hospital' in o for o in outcomes_lower),\n",
    "        'death': any('death' in o for o in outcomes_lower)\n",
    "    }\n",
    "\n",
    "outcome_features = df['outcomes'].apply(extract_outcome_severity)\n",
    "df['is_serious'] = outcome_features.apply(lambda x: x['serious'])\n",
    "df['is_hospitalization'] = outcome_features.apply(lambda x: x['hospitalization'])\n",
    "df['is_death'] = outcome_features.apply(lambda x: x['death'])\n",
    "\n",
    "print(f\"\\nOutcome severity distribution:\")\n",
    "print(f\"Serious outcomes: {df['is_serious'].sum():,} ({df['is_serious'].mean()*100:.1f}%)\")\n",
    "print(f\"Hospitalizations: {df['is_hospitalization'].sum():,} ({df['is_hospitalization'].mean()*100:.1f}%)\")\n",
    "print(f\"Deaths: {df['is_death'].sum():,} ({df['is_death'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Time Series by Reaction Type\n",
    "\n",
    "This is the core dataset for anomaly detection - we'll create time series for each reaction type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top reactions to focus on\n",
    "all_reactions = []\n",
    "for reactions in df['reactions_clean']:\n",
    "    all_reactions.extend(reactions)\n",
    "\n",
    "reaction_counts = Counter(all_reactions)\n",
    "top_n = 20  # Focus on top 20 reactions\n",
    "top_reactions = [r for r, _ in reaction_counts.most_common(top_n)]\n",
    "\n",
    "print(f\"Focusing on top {top_n} reactions:\")\n",
    "for i, (reaction, count) in enumerate(reaction_counts.most_common(top_n), 1):\n",
    "    print(f\"{i:2d}. {reaction:40s} {count:8,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series for each reaction at different frequencies\n",
    "def create_reaction_timeseries(df, reactions, freq='D'):\n",
    "    \"\"\"\n",
    "    Create time series dataframe for specified reactions\n",
    "    \n",
    "    Args:\n",
    "        df: Source dataframe\n",
    "        reactions: List of reaction names\n",
    "        freq: Frequency ('D'=daily, 'W'=weekly, 'M'=monthly)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with date index and columns for each reaction\n",
    "    \"\"\"\n",
    "    # Create a complete date range\n",
    "    date_range = pd.date_range(\n",
    "        start=df['date_started'].min(),\n",
    "        end=df['date_started'].max(),\n",
    "        freq=freq\n",
    "    )\n",
    "    \n",
    "    # Initialize result dataframe\n",
    "    result = pd.DataFrame(index=date_range)\n",
    "    \n",
    "    # Add count for each reaction\n",
    "    for reaction in reactions:\n",
    "        # Filter records containing this reaction\n",
    "        mask = df['reactions_clean'].apply(lambda x: reaction in x)\n",
    "        reaction_df = df[mask].copy()\n",
    "        \n",
    "        # Aggregate by time period\n",
    "        counts = reaction_df.groupby(pd.Grouper(key='date_started', freq=freq)).size()\n",
    "        \n",
    "        # Add to result, filling missing dates with 0\n",
    "        result[reaction] = counts.reindex(date_range, fill_value=0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create daily time series\n",
    "print(\"Creating daily time series...\")\n",
    "ts_daily = create_reaction_timeseries(df, top_reactions, freq='D')\n",
    "print(f\"Shape: {ts_daily.shape}\")\n",
    "print(f\"Date range: {ts_daily.index.min()} to {ts_daily.index.max()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(ts_daily.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weekly time series (often better for outbreak detection)\n",
    "print(\"Creating weekly time series...\")\n",
    "ts_weekly = create_reaction_timeseries(df, top_reactions, freq='W')\n",
    "print(f\"Shape: {ts_weekly.shape}\")\n",
    "print(f\"Date range: {ts_weekly.index.min()} to {ts_weekly.index.max()}\")\n",
    "print(f\"\\nSample:\")\n",
    "print(ts_weekly.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize time series for top 5 reactions\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 15))\n",
    "\n",
    "for idx, reaction in enumerate(top_reactions[:5]):\n",
    "    axes[idx].plot(ts_weekly.index, ts_weekly[reaction], linewidth=2)\n",
    "    axes[idx].set_title(f'Weekly Time Series: {reaction.title()}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].fill_between(ts_weekly.index, ts_weekly[reaction], alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Features for Time Series\n",
    "\n",
    "Calculate rolling statistics that will be useful for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(ts_df, window=7):\n",
    "    \"\"\"\n",
    "    Add rolling statistical features to time series\n",
    "    \n",
    "    Args:\n",
    "        ts_df: Time series dataframe\n",
    "        window: Rolling window size\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with additional feature columns\n",
    "    \"\"\"\n",
    "    result = ts_df.copy()\n",
    "    \n",
    "    for col in ts_df.columns:\n",
    "        # Rolling mean\n",
    "        result[f'{col}_rolling_mean'] = ts_df[col].rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # Rolling std\n",
    "        result[f'{col}_rolling_std'] = ts_df[col].rolling(window=window, min_periods=1).std()\n",
    "        \n",
    "        # Z-score (standardized value)\n",
    "        mean = result[f'{col}_rolling_mean']\n",
    "        std = result[f'{col}_rolling_std']\n",
    "        result[f'{col}_zscore'] = (ts_df[col] - mean) / (std + 1e-10)  # Add small value to avoid division by zero\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Add features to daily time series\n",
    "print(\"Adding rolling features to daily time series...\")\n",
    "ts_daily_featured = add_rolling_features(ts_daily, window=7)\n",
    "print(f\"Original columns: {len(ts_daily.columns)}\")\n",
    "print(f\"With features: {len(ts_daily_featured.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rolling statistics for one reaction\n",
    "reaction_example = top_reactions[0]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "# Original count\n",
    "axes[0].plot(ts_daily.index, ts_daily[reaction_example], label='Actual Count', alpha=0.6)\n",
    "axes[0].plot(ts_daily_featured.index, ts_daily_featured[f'{reaction_example}_rolling_mean'], \n",
    "            label='7-day Rolling Mean', linewidth=2, color='red')\n",
    "axes[0].set_title(f'{reaction_example.title()} - Count with Rolling Mean', fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Rolling standard deviation\n",
    "axes[1].plot(ts_daily_featured.index, ts_daily_featured[f'{reaction_example}_rolling_std'], \n",
    "            color='orange', linewidth=2)\n",
    "axes[1].set_title(f'{reaction_example.title()} - Rolling Standard Deviation', fontweight='bold')\n",
    "axes[1].set_ylabel('Std Dev')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Z-score\n",
    "axes[2].plot(ts_daily_featured.index, ts_daily_featured[f'{reaction_example}_zscore'], \n",
    "            color='purple', linewidth=1.5)\n",
    "axes[2].axhline(y=3, color='red', linestyle='--', label='3σ threshold')\n",
    "axes[2].axhline(y=-3, color='red', linestyle='--')\n",
    "axes[2].axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "axes[2].set_title(f'{reaction_example.title()} - Z-Score', fontweight='bold')\n",
    "axes[2].set_ylabel('Z-Score')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split\n",
    "\n",
    "For time series, we use temporal splits to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by date - use last 20% for testing\n",
    "split_date = ts_daily.index[int(len(ts_daily) * 0.8)]\n",
    "\n",
    "# Daily splits\n",
    "ts_daily_train = ts_daily[ts_daily.index < split_date]\n",
    "ts_daily_test = ts_daily[ts_daily.index >= split_date]\n",
    "\n",
    "# Weekly splits\n",
    "split_date_weekly = ts_weekly.index[int(len(ts_weekly) * 0.8)]\n",
    "ts_weekly_train = ts_weekly[ts_weekly.index < split_date_weekly]\n",
    "ts_weekly_test = ts_weekly[ts_weekly.index >= split_date_weekly]\n",
    "\n",
    "print(\"Daily time series split:\")\n",
    "print(f\"  Train: {ts_daily_train.index.min()} to {ts_daily_train.index.max()} ({len(ts_daily_train)} records)\")\n",
    "print(f\"  Test:  {ts_daily_test.index.min()} to {ts_daily_test.index.max()} ({len(ts_daily_test)} records)\")\n",
    "\n",
    "print(\"\\nWeekly time series split:\")\n",
    "print(f\"  Train: {ts_weekly_train.index.min()} to {ts_weekly_train.index.max()} ({len(ts_weekly_train)} records)\")\n",
    "print(f\"  Test:  {ts_weekly_test.index.min()} to {ts_weekly_test.index.max()} ({len(ts_weekly_test)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save cleaned raw data\n",
    "df.to_parquet('../data/processed/cleaned_data.parquet', compression='snappy', index=False)\n",
    "print(\"Saved: cleaned_data.parquet\")\n",
    "\n",
    "# Save time series data\n",
    "ts_daily.to_parquet('../data/processed/timeseries_daily.parquet')\n",
    "print(\"Saved: timeseries_daily.parquet\")\n",
    "\n",
    "ts_weekly.to_parquet('../data/processed/timeseries_weekly.parquet')\n",
    "print(\"Saved: timeseries_weekly.parquet\")\n",
    "\n",
    "# Save train/test splits\n",
    "ts_daily_train.to_parquet('../data/processed/timeseries_daily_train.parquet')\n",
    "ts_daily_test.to_parquet('../data/processed/timeseries_daily_test.parquet')\n",
    "print(\"Saved: daily train/test splits\")\n",
    "\n",
    "ts_weekly_train.to_parquet('../data/processed/timeseries_weekly_train.parquet')\n",
    "ts_weekly_test.to_parquet('../data/processed/timeseries_weekly_test.parquet')\n",
    "print(\"Saved: weekly train/test splits\")\n",
    "\n",
    "# Save list of top reactions for reference\n",
    "import json\n",
    "with open('../data/processed/top_reactions.json', 'w') as f:\n",
    "    json.dump(top_reactions, f, indent=2)\n",
    "print(\"Saved: top_reactions.json\")\n",
    "\n",
    "print(\"\\n✓ All preprocessed data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCleaned Dataset:\")\n",
    "print(f\"  Total records: {len(df):,}\")\n",
    "print(f\"  Date range: {df['date_started'].min()} to {df['date_started'].max()}\")\n",
    "print(f\"  Unique reactions: {len(reaction_counts):,}\")\n",
    "print(f\"  Focus reactions: {len(top_reactions)}\")\n",
    "\n",
    "print(f\"\\nTime Series Data:\")\n",
    "print(f\"  Daily time series: {ts_daily.shape}\")\n",
    "print(f\"  Weekly time series: {ts_weekly.shape}\")\n",
    "print(f\"  Daily train/test: {ts_daily_train.shape} / {ts_daily_test.shape}\")\n",
    "print(f\"  Weekly train/test: {ts_weekly_train.shape} / {ts_weekly_test.shape}\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(\"  1. Implement baseline anomaly detection (Z-score, moving average)\")\n",
    "print(\"  2. Train Isolation Forest models\")\n",
    "print(\"  3. Implement ARIMA/Prophet models\")\n",
    "print(\"  4. Compare model performance\")\n",
    "print(\"  5. Validate against known FDA recalls\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
